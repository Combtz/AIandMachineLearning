{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNSEFWFCHUplCj8KvVLf3na"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Assignment 2 2024"],"metadata":{"id":"EtlT7ZL5J8h2"}},{"cell_type":"markdown","source":["In the first section of this assignment, your task is to perform a sentiment analysis task on the Twitter dataset, train.csv, provided using the BERT model. Your job is to answer each question carrying a particular mark. The BERT model can be imported from the huggingface library. Your codes have to be executable in Google Colab. The dataset contains around 32K tweets which are labeled based on having racist or sexist content.  "],"metadata":{"id":"jxxflyckKKAM"}},{"cell_type":"markdown","source":["### Mount your google drive and and change your path"],"metadata":{"id":"333zkvSRLUL0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1ZSag1KJ7w8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708564045033,"user_tz":-630,"elapsed":32341,"user":{"displayName":"Mahardhika Pratama","userId":"18186641230014948214"}},"outputId":"b443ab3a-8772-46c0-afe6-7517f06060c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/MachineLearning/Assignment 2/Sentiment-Analysis-with-Twitter-main/Sentiment-Analysis-with-Twitter-main')"],"metadata":{"id":"QdByOLFeLThW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Install the huggingface module"],"metadata":{"id":"2FGR09J8L7cU"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"_YB9ja9NMIpc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708564065224,"user_tz":-630,"elapsed":5887,"user":{"displayName":"Mahardhika Pratama","userId":"18186641230014948214"}},"outputId":"9c2f3380-099e-4001-dc72-5a06f673047b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}]},{"cell_type":"markdown","source":["### Install the preprocessing text"],"metadata":{"id":"9f90Wg1jMozk"}},{"cell_type":"code","source":["!pip install keras-preprocessing"],"metadata":{"id":"xwlyvkT7MtQb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708564073958,"user_tz":-630,"elapsed":5754,"user":{"displayName":"Mahardhika Pratama","userId":"18186641230014948214"}},"outputId":"56f76cd9-9902-4c47-aca5-0de42eddfcbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras-preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.25.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.16.0)\n","Installing collected packages: keras-preprocessing\n","Successfully installed keras-preprocessing-1.1.2\n"]}]},{"cell_type":"markdown","source":["### 1. Import all relevant modules here [5 marks]."],"metadata":{"id":"oCef_6XqLna9"}},{"cell_type":"markdown","source":["##### place all relevant libraries into the following cell"],"metadata":{"id":"2G7b9UIlwkOY"}},{"cell_type":"code","source":[],"metadata":{"id":"6wgks8TPwr1Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Load your dataset and show the first five rows of the dataset [5 marks]."],"metadata":{"id":"nLyP7-PBNiUU"}},{"cell_type":"markdown","source":["##### you need to use Pandas in order to load the dataset"],"metadata":{"id":"EtFcWCGywxUw"}},{"cell_type":"code","source":[],"metadata":{"id":"0Ns5ngIJw5Ow"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Given the following function to clean the raw text of your dataset, clean your dataset and show the first five rows of the clean dataset [5 marks]."],"metadata":{"id":"7jGAWQJKNsyM"}},{"cell_type":"markdown","source":["##### apply the following function to clean your dataset."],"metadata":{"id":"bD4mv3yiw9bQ"}},{"cell_type":"code","source":["# Cleaning Raw tweets\n","def clean_text(text):\n","\n","    #remove emails\n","    text = ' '.join([i for i in text.split() if '@' not in i])\n","\n","    #remove web address\n","    text = re.sub('http[s]?://\\S+', '', text)\n","\n","    #remove punctuations\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","\n","    #Filter to allow only alphabets\n","    text = re.sub(r'[^a-zA-Z0-9\\']', ' ', text)\n","\n","    #Remove Unicode characters\n","    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n","\n","    #remove double spaces\n","    text = re.sub('\\s+', ' ', text)\n","\n","    return text"],"metadata":{"id":"wJQRQiifOF6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aPX3B0PLxDmg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Preprocess your text dataset to be compatible to BERT model. Set MAX_LEN to 128 [20 marks]."],"metadata":{"id":"mEUX0oe1OhDM"}},{"cell_type":"markdown","source":["##### the preprocesing step is to tokenize your dataset such that it is fit to input BERT"],"metadata":{"id":"2gUxR9jCxI_P"}},{"cell_type":"code","source":[],"metadata":{"id":"l_ZTwHmmxTmH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Split the dataset to the training set and the testing set with the ratio of 90% for the training dataset and 10% for the testing dataset [5 marks]."],"metadata":{"id":"9uoRAtviUG7j"}},{"cell_type":"markdown","source":["##### Both data and attention mask need to be split"],"metadata":{"id":"jTLVQRh_xUxg"}},{"cell_type":"code","source":[],"metadata":{"id":"m57OFKsCxd0o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Create the dataloader of the datasets with the batch size of 32 [5 marks]."],"metadata":{"id":"pptk7y2DUkf1"}},{"cell_type":"markdown","source":["##### The dataloader can be created in the tensorflow"],"metadata":{"id":"gfoLWA7Dximo"}},{"cell_type":"code","source":[],"metadata":{"id":"TbGOzdBUxnGY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7. Create the BERT model from the hugging face library [5 marks]."],"metadata":{"id":"0_G2v2MzU7Gc"}},{"cell_type":"markdown","source":["##### You can craft your BERT manually or directly use a pre-developed model for text classification"],"metadata":{"id":"Wo5HcC4DxqSo"}},{"cell_type":"code","source":[],"metadata":{"id":"Nb_7HsxPxpeY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8. Prepare the training environments for your BERT model with the cross entropy loss function and the ADAM optimizer [10 marks]"],"metadata":{"id":"zsIzLzjpVM0b"}},{"cell_type":"markdown","source":["##### prepare your loss and learning rate"],"metadata":{"id":"vEg5z0Ncx33A"}},{"cell_type":"code","source":[],"metadata":{"id":"GdF_VzLHyH_5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9. Train your BERT model with 2 epochs [20 marks]."],"metadata":{"id":"N83rzrEWVo0T"}},{"cell_type":"markdown","source":["##### you need to create your training loop here"],"metadata":{"id":"j-rxrGmKyJPY"}},{"cell_type":"code","source":[],"metadata":{"id":"jf4iNDXTyNXB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 10. Evaluate your model with the testing dataset and calculate accuracy, precision, recall and F1 score [20 marks]."],"metadata":{"id":"P9HjRffXWHZ7"}},{"cell_type":"markdown","source":["##### evaluation need to be done inside the training loop"],"metadata":{"id":"2Dav-7itySzo"}},{"cell_type":"code","source":[],"metadata":{"id":"Q7D5opcKyX84"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the second part of the assignment, you are suppossed to perform sentiment analysis on the smile-annotations-final.csv. It differs from the first part of the assignment where this dataset features a multi-class classification problem."],"metadata":{"id":"wbT91XqPH-5Z"}},{"cell_type":"markdown","source":["### 11. Load the dataset and show the first few rows of the dataset [5 marks]."],"metadata":{"id":"zPj5qW78Ie4h"}},{"cell_type":"markdown","source":["##### you need to use Pandas to load the dataset"],"metadata":{"id":"CemTStVdyge4"}},{"cell_type":"code","source":[],"metadata":{"id":"p-IMAqPlylBA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 12. Identify the number of classes in the dataset [5 marks]."],"metadata":{"id":"vzHHjQ4eI-Mi"}},{"cell_type":"markdown","source":["##### you can identify the unique category in your dataset"],"metadata":{"id":"5AiwYHTyymKQ"}},{"cell_type":"code","source":[],"metadata":{"id":"JB1nh5YlyvfQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 13. Form a six-classes classification problem that consists of happy, not-relevant, angry, surprise, sad and disgust [5 marks]."],"metadata":{"id":"Gug4iBXHJP0q"}},{"cell_type":"markdown","source":["##### Remove irrelevant classes"],"metadata":{"id":"-0Cca6-4ywpp"}},{"cell_type":"code","source":[],"metadata":{"id":"TH8YYD-cyz8o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 14. Replace text labels with numeric labels [5 marks]."],"metadata":{"id":"1TlZR_WdJ9ry"}},{"cell_type":"code","source":[],"metadata":{"id":"RecllBABy6Qo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 15. Remove irrelevant characters in the text and show the first few rows of the clean dataset [5 marks]."],"metadata":{"id":"Y1JlGsvIKJCR"}},{"cell_type":"markdown","source":["##### you can use the same function given in the first part of the assignment"],"metadata":{"id":"EG8DiTjPy7-g"}},{"cell_type":"code","source":[],"metadata":{"id":"eSQnQL6pzEAJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 16. Preprocess your text dataset to be compatible to BERT model. Set MAX_LEN to 128 [20 marks]."],"metadata":{"id":"iqM9lRHQKkiq"}},{"cell_type":"markdown","source":["##### tokenize your dataset such that it is ready for BERT to use"],"metadata":{"id":"cORAgE5l1jRg"}},{"cell_type":"code","source":[],"metadata":{"id":"POeLG71w1vP4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 17. Split the dataset to the training set and the testing set with the ratio of 90% for the training dataset and 10% for the testing dataset [5 marks]."],"metadata":{"id":"lV9cyQEmKrQq"}},{"cell_type":"markdown","source":["##### Do not forget the attention mask to be split"],"metadata":{"id":"KRGI38-21wLo"}},{"cell_type":"code","source":[],"metadata":{"id":"o-faDZqk128o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 18. Create the dataloader of the datasets with the batch size of 32 [5 marks]."],"metadata":{"id":"qRygfyXlK82q"}},{"cell_type":"markdown","source":["##### Dataloader can be created in the tensorflow"],"metadata":{"id":"mp5dDJzc14BQ"}},{"cell_type":"code","source":[],"metadata":{"id":"LZK18EVS2Bbw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 19. Prepare the training environments for your BERT model with the cross entropy loss function and the ADAM optimizer [10 marks]"],"metadata":{"id":"TB1xInzcLLDy"}},{"cell_type":"markdown","source":["##### Prepare the loss function, the learning rate and the optimizer"],"metadata":{"id":"GZWxLwVt2CZI"}},{"cell_type":"code","source":[],"metadata":{"id":"u2G7hWV62LTI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 20. Train your BERT model with 2 epochs [20 marks]."],"metadata":{"id":"iG6hEvyaLvKy"}},{"cell_type":"markdown","source":["##### Create your training loop"],"metadata":{"id":"OAu4XPh22MX6"}},{"cell_type":"code","source":[],"metadata":{"id":"l7eiKO1D2P0Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 21. Evaluate your model with the testing dataset and calculate accuracy [20 marks]."],"metadata":{"id":"0LkvlRarL7HB"}},{"cell_type":"markdown","source":["##### Evaluation needs to be done inside the training loop"],"metadata":{"id":"h8t_PBkk2Qkp"}},{"cell_type":"code","source":[],"metadata":{"id":"FrCzUa_H2WEQ"},"execution_count":null,"outputs":[]}]}